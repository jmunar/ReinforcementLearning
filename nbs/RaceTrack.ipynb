{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Revise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Track used for tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track = [\n",
    "    0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 3\n",
    "    0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3\n",
    "    0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3\n",
    "    0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3\n",
    "    1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3\n",
    "    1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3\n",
    "    1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0\n",
    "    1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
    "    1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
    "    1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
    "    1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
    "    1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
    "    1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
    "    1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
    "    0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
    "    0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
    "    0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
    "    0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
    "    0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
    "    0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
    "    0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
    "    0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
    "    0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
    "    0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
    "    0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
    "    0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
    "    0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
    "    0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
    "    0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
    "    0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
    "    0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
    "    0 0 0 2 2 2 2 2 2 0 0 0 0 0 0 0 0\n",
    "    ][end:-1:1,:]\n",
    "\n",
    "nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Off-policy Monte Carlo optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module Opts\n",
    "\n",
    "using ReinforcementLearning.RaceTrack: Action, Game, PlayerDeterministic, PlayerRandom, State, action_valid, nactions, nstates, play_game\n",
    "\n",
    "function optimize_policy_mc_off_policy(game::Game, player_b::PlayerRandom, Γ::Float64 = 1., n_games::Int = 10000)::Array{Action, 1}\n",
    "    \n",
    "    states = [State(size(game.track)..., s_idx) for s_idx in 1:nstates(game.track)]\n",
    "    actions = [Action(a_idx) for a_idx in 1:nactions()]\n",
    "\n",
    "    q = zeros(nstates(game.track), nactions())\n",
    "    c = zeros(nstates(game.track), nactions())\n",
    "    w_norm = zeros(nstates(game.track))\n",
    "    \n",
    "    for (s_idx, s) in enumerate(states)\n",
    "        for (a_idx, a) in enumerate(actions)\n",
    "            v = action_valid(s, a)\n",
    "            w_norm[s_idx] += v\n",
    "            if !v\n",
    "                q[s_idx, a_idx] = -Inf\n",
    "            else\n",
    "                q[s_idx, a_idx] = -500.\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    action_index_range = 1:nactions()\n",
    "    π = [rand(action_index_range[q_row .== maximum(q_row)]) for q_row in eachrow(q)]\n",
    "    \n",
    "    for game_index in 1:n_games\n",
    "        g = 0.\n",
    "        w = 1.\n",
    "\n",
    "        nsteps = play_game(game, player_b)\n",
    "        \n",
    "        for step in nsteps-1:-1:1\n",
    "            s, a, r = game.episode_states[step], game.episode_actions[step], game.episode_rewards[step]\n",
    "            g = Γ * g + r\n",
    "            c[s, a] += w\n",
    "            q[s, a] += (w / c[s, a]) * (g - q[s, a])\n",
    "            \n",
    "            m = \n",
    "            π[s] = rand(action_index_range[q[s, :] .== maximum(q[s, :])])\n",
    "            if π[s] != a\n",
    "                break\n",
    "            end\n",
    "            \n",
    "            w /= w_norm[s]\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return map(Action, π)\n",
    "end\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ReinforcementLearning.RaceTrack: Game, PlayerDeterministic, PlayerRandom, play_game\n",
    "\n",
    "for Γ in [0.5, 0.9, 1.0]\n",
    "    for max_n_iter in [10000, 20000, 50000, 100000, 200000]\n",
    "        game = Game(track, 0.1)\n",
    "        player_b = PlayerRandom(size(track)...)\n",
    "        policy = Opts.optimize_policy_mc_off_policy(game, player_b, Γ, max_n_iter)\n",
    "\n",
    "        game = Game(track, 0.)\n",
    "        player_t = PlayerDeterministic(policy)\n",
    "        println(\"Γ: \", Γ, \", optimization iterations: \", max_n_iter)\n",
    "        println(\"Average episode length: \", sum([play_game(game, player_t) for _ in 1:100000]) / 100000)\n",
    "    end\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.1",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
